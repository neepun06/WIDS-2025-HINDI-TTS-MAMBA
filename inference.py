# inference.py

import torch
import numpy as np
from models.mamba_tts import MambaTTS
from preprocess.text import CharTokenizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load tokenizer
texts = []
with open("data/hindi/metadata/transcripts.txt", encoding="utf-8") as f:
    for line in f:
        parts = line.split('"')
        if len(parts) > 1:
            texts.append(parts[1])

tokenizer = CharTokenizer(texts)

# Load model
model = MambaTTS(vocab_size=len(tokenizer)).to(device)
ckpt = torch.load("checkpoints/best_model.pt", map_location=device)
model.load_state_dict(ckpt["model_state"])
model.eval()

# Long inference text (important!)
text = (
    "‡§∞‡§æ‡§Æ ‡§ú‡§Ç‡§ó‡§≤ ‡§Æ‡•á‡§Ç ‡§ó‡§è ‡§î‡§∞ ‡§µ‡§π‡§æ‡§Å ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§ã‡§∑‡§ø ‡§∏‡•á ‡§≠‡•á‡§Ç‡§ü ‡§ï‡•Ä "
    "‡§î‡§∞ ‡§ú‡•ç‡§û‡§æ‡§® ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§î‡§∞ ‡§Ö‡§™‡§®‡•á ‡§ú‡•Ä‡§µ‡§® ‡§ï‡§æ ‡§Æ‡§æ‡§∞‡•ç‡§ó ‡§∏‡§Æ‡§ù‡§æ"
)

tokens = torch.tensor(tokenizer.encode(text)).unsqueeze(0).to(device)

with torch.no_grad():
    mel = model(tokens)
    mel = mel.squeeze(0)

# üîß Stretch time axis (CRUCIAL for audibility)
mel = mel.repeat_interleave(4, dim=0)

np.save("output_mel.npy", mel.cpu().numpy())
print("Saved mel:", mel.shape)
